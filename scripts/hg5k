#!/usr/bin/env python

import os
import sys
import threading

from argparse import ArgumentParser, RawTextHelpFormatter

from execo.action import Get, Put, TaktukRemote
from execo.host import Host
from execo.log import style
from execo.process import SshProcess
from execo_engine import logger

from hadoop_g5k.cluster import HadoopCluster
from hadoop_g5k.cluster_v2 import HadoopV2Cluster
from hadoop_g5k.objects import HadoopJarJob
from hadoop_g5k.util import generate_hosts, generate_new_id, ClusterType, \
    get_default_id, cluster_exists, deserialize_cluster, remove_cluster, \
    serialize_cluster


if __name__ == "__main__":

    prog = "hg5k"
    description = "This tool helps you to manage a Hadoop cluster in Grid5000."
    parser = ArgumentParser(prog=prog,
                            description=description,
                            formatter_class=RawTextHelpFormatter,
                            add_help=False)

    actions = parser.add_argument_group(style.host("General options"),
                                        "Options to be used generally "
                                        "with Hadoop actions.")

    actions.add_argument("-h", "--help",
                         action="help",
                         help="Show this help message and exit")

    actions.add_argument("--id",
                         action="store",
                         nargs=1,
                         metavar="ID",
                         help="The identifier of the cluster. If not indicated"
                                ", last used cluster will be used (if any)")

    actions.add_argument("--node",
                         action="store",
                         nargs=1,
                         metavar="NODE",
                         help="Node where the action will be executed. Applies"
                                " only to --execute and --jarjob")

    actions.add_argument("--libjars",
                         action="store",
                         nargs="+",
                         metavar="LIB_JARS",
                         help="A list of libraries to be used in job execution"
                                ". Applies only to --jarjob")

    verbose_group = actions.add_mutually_exclusive_group()

    verbose_group.add_argument("-v", "--verbose",
                               dest="verbose",
                               action="store_true",
                               help="Run in verbose mode")

    verbose_group.add_argument("-q", "--quiet",
                               dest="quiet",
                               action="store_true",
                               help="Run in quiet mode")

    object_group = parser.add_argument_group(style.host("Object management "
                                                        "options"),
                                             "Options to create and destroy "
                                             "hadoop cluster objects.")

    object_mutex_group = object_group.add_mutually_exclusive_group()

    object_mutex_group.add_argument("--create",
                                    metavar="MACHINELIST",
                                    nargs=1,
                                    action="store",
                                    help="Create the cluster object with the "
                                    "nodes in MACHINELIST file")

    object_mutex_group.add_argument("--delete",
                                    dest="delete",
                                    action="store_true",
                                    help="Remove all files used by the cluster")

    object_group.add_argument("--version",
                              dest="version",
                              nargs=1,
                              action="store",
                              help="Hadoop version to be used")

    object_group.add_argument("--properties",
                              dest="properties",
                              nargs=1,
                              action="store",
                              help="File containing the properties to be used "
                              "(INI file). Applies only to --create")

    object_group.add_argument("--bootstrap",
                              metavar="HADOOP_TAR",
                              nargs=1,
                              action="store",
                              help="Install Hadoop in the cluster nodes taking"
                                   " into account the specified properties.\n"
                                   "HADOOP_TAR defines the path of the .tar.gz"
                                   " file containing Hadoop binaries.")

    actions = parser.add_argument_group(style.host("Hadoop actions"),
                                        "Actions to execute in the Hadoop "
                                        "cluster. Several options can be "
                                        "indicated at the same time.\n"
                                        "The order of execution is fixed no "
                                        "matter the order used in the "
                                        "arguments: it follows the order\n"
                                        "of the options.")

    actions.add_argument("--initialize",
                         dest="initialize",
                         action="store_true",
                         help="Initialize cluster: Copy configuration and "
                         "format dfs")

    actions.add_argument("--changeconf",
                         action="store",
                         nargs="+",
                         metavar="NAME=VALUE",
                         help="Change given configuration variables")

    actions.add_argument("--start",
                         dest="start",
                         action="store_true",
                         help="Start the NameNode and JobTracker")

    actions.add_argument("--putindfs",
                         action="store",
                         nargs="+",
                         metavar="PATH",
                         help="Copy a set of local paths into the remote path "
                         "in dfs")

    actions.add_argument("--getfromdfs",
                         action="store",
                         nargs=2,
                         metavar=("DFS_PATH", "LOCAL_PATH"),
                         help="Copy a remote path in dfs into the specified "
                         "local path")

    actions.add_argument("--execute",
                         action="store",
                         nargs=1,
                         metavar="COMMAND",
                         help="Execute a Hadoop command")

    actions.add_argument("--jarjob",
                         action="store",
                         nargs="+",
                         metavar=("LOCAL_JAR_PATH", "PARAM"),
                         help="Copy the jar file and execute it with the "
                         "specified parameters")

    actions.add_argument("--copyhistory",
                         action="store",
                         nargs="+",
                         metavar=("LOCAL_PATH", "JOB_ID"),
                         help="Copy history to the specified path.\n"
                         "If a list of job ids is given, just copy the "
                         "stats of those jobs.")

    actions.add_argument("--stop",
                         dest="stop",
                         action="store_true",
                         help="Stop the NameNode and JobTracker")

    actions.add_argument("--clean",
                         dest="clean",
                         action="store_true",
                         help="Remove Hadoop logs and clean the dfs")

    queries = parser.add_argument_group(style.host("Hadoop queries"),
                                        "Set of queries that can be executed "
                                        "in Hadoop.")

    queries.add_argument("--state",
                         action="store",
                         nargs="?",
                         const="general",
                         metavar="general | files | dfs | dfsblocks | mrjobs",
                         help="Show the cluster state. The output depends on "
                         "optional argument:\n"
                         "  general    Show general cluster state"
                         " (default option)\n"
                         "  files      Show dfs file hierarchy\n" +
                         "  dfs        Show filesystem state\n" +
                         "  dfsblocks  Show dfs blocks information\n" +
                         "  mrjobs     Show mapreduce state\n")

    args = parser.parse_args()

    # Get id
    if args.id:
        hc_id = int(args.id[0])
    else:
        if args.create:
            hc_id = generate_new_id(ClusterType.HADOOP)
        else:
            hc_id = get_default_id(ClusterType.HADOOP)
            if not hc_id:
                logger.error("There is no available cluster. You must create a"
                             " new one")
                sys.exit(os.EX_DATAERR)

    logger.info("Using id = " + str(hc_id) + " (HADOOP)")

    verbose = True
    if args.quiet:
        verbose = False

    # Check node specification
    node_host = None
    if args.node:
        if not (args.execute or args.jarjob):
            logger.warn("--node only applies to --execute or --jarjob")
        else:
            node_host = Host(args.node[0])

    # Create or load object
    if args.create:

        if cluster_exists(ClusterType.HADOOP, hc_id):
            logger.error("There is a hadoop cluster with that id. You must "
                         "remove it before or chose another id")
            sys.exit(os.EX_DATAERR)

        hosts = generate_hosts(args.create[0])

        if args.version:
            if args.version[0] == "0" or args.version[0].startswith("0."):
                hadoop_class = HadoopCluster
            elif args.version[0] == "1" or args.version[0].startswith("1."):
                hadoop_class = HadoopCluster
            elif args.version[0] == "2" or args.version[0].startswith("2."):
                hadoop_class = HadoopV2Cluster
            else:
                logger.error("Unknown hadoop version")
                sys.exit(os.EX_DATAERR)
        else:
            hadoop_class = HadoopCluster

        if args.properties:
            hc = hadoop_class(hosts, None, args.properties[0])
        else:
            hc = hadoop_class(hosts)

    elif args.delete:

        # Clean
        hc = deserialize_cluster(ClusterType.HADOOP, hc_id)
        if hc.initialized:
            logger.warn("The cluster needs to be cleaned before removed.")
            hc.clean()

        # Remove hc dump file
        logger.info("Removing hc dump file from cluster")
        remove_cluster(ClusterType.HADOOP, hc_id)

        sys.exit(os.EX_OK)
    else:
        # Deserialize
        hc = deserialize_cluster(ClusterType.HADOOP, hc_id)

    # Execute options
    if args.bootstrap:
        hc.bootstrap(args.bootstrap[0])

    if args.initialize:
        hc.initialize()

    if args.changeconf:
        params = {}
        for assig in args.changeconf:
            parts = assig.split("=")
            params[parts[0]] = parts[1]

        hc.change_conf(params)

    if args.start:
        hc.start_and_wait()

    if args.putindfs:
        local_paths = args.putindfs[:-1]
        dest = args.putindfs[-1]

        for f in local_paths:
            if not os.path.exists(f):
                logger.error("Local path " + f + " does not exist")
                sys.exit(os.EX_NOINPUT)

        # Create dest directories if needed
        if hc.get_version().startswith("Hadoop 2."):
            hc.execute("fs -mkdir -p " + dest, verbose=False)

        # Define and create temp dir
        tmp_dir = "/tmp" + dest
        hosts = hc.hosts
        actionRemove = TaktukRemote("rm -rf " + tmp_dir, hosts)
        actionRemove.run()
        actionCreate = TaktukRemote("mkdir -p " + tmp_dir, hosts)
        actionCreate.run()

        def copy_function(host, files_to_copy):
            action_copy = Put([host], files_to_copy, tmp_dir)
            action_copy.run()

            for f in files_to_copy:
                src_file = os.path.join(tmp_dir, os.path.basename(f))

                hc.execute("fs -put " + src_file + " " +
                           os.path.join(dest, os.path.basename(src_file)),
                           host, True, False)

        # Assign files to hosts
        files_per_host = [[]] * len(hosts)
        for idx in range(0, len(hosts)):
            files_per_host[idx] = local_paths[idx::len(hosts)]

        # Create threads and launch them
        logger.info("Copying files in parallel into " + str(len(hosts)) +
                    " hosts")

        threads = []
        for idx, h in enumerate(hosts):
            if files_per_host[idx]:
                t = threading.Thread(target=copy_function,
                                     args=(h, files_per_host[idx]))
                t.start()
                threads.append(t)

        # Wait for the threads to finish
        for t in threads:
            t.join()

    if args.getfromdfs:
        remote_path = args.getfromdfs[0]
        local_path = args.getfromdfs[1]

        tmp_dir = "/tmp"
        # Remove file in tmp dir if exists
        proc = SshProcess("rm -rf " +
                          os.path.join(tmp_dir, os.path.basename(remote_path)),
                          hc.master)
        proc.run()

        # Get files in master
        hc.execute("fs -get " + remote_path + " " + tmp_dir, verbose=False)

        # Copy files from master
        action = Get([hc.master],
                     [os.path.join(tmp_dir, os.path.basename(remote_path))],
                     local_path)
        action.run()

    if args.execute:
        if node_host:
            hc.execute(args.execute[0], node_host, verbose=verbose)
        else:
            hc.execute(args.execute[0], verbose=verbose)

    if args.jarjob:
        if not node_host:
            node_host = None
        if args.libjars:
            libjars = args.libjars
        else:
            libjars = None

        job = HadoopJarJob(args.jarjob[0], args.jarjob[1:], libjars)

        hc.execute_jar(job, verbose=verbose, node=node_host)
        if job.success:
            print "Job with id " + job.job_id + " finished successfully"
        else:
            print "Job finished with errors"

    if args.copyhistory:
        hc.copy_history(args.copyhistory[0], args.copyhistory[1:])

    if args.stop:
        hc.stop()

    if args.clean:
        hc.clean()

    if args.state:
        if args.state == "general":

            logger.info("-"*55)
            logger.info(style.user2("Hadoop Cluster with ID " + str(hc_id)))
            logger.info(style.user1("    Version: ") + hc.get_version())
            logger.info(style.user1("    Master: ") + str(hc.master))
            logger.info(style.user1("    Hosts: ") + str(hc.hosts))
            logger.info(style.user1("    Topology: "))
            for h in hc.hosts:
                logger.info("        " + str(h) + " -> " +
                            str(hc.topology.get_rack(h)))
            if hc.initialized:
                if hc.running:
                    logger.info("The cluster is " + style.user3("running"))
                else:
                    logger.info("The cluster is " + style.user3("stopped"))
            else:
                logger.info("The cluster is not " + style.user3("initialized"))
            logger.info("-"*55)

        elif args.state == "files":
            if hc.get_version().startswith("Hadoop 2."):
                (stdout, stderr) = hc.execute("fs -ls -R /", verbose=False)
            else:
                (stdout, stderr) = hc.execute("fs -lsr /", verbose=False)
            print ""
            for line in stdout.splitlines():
                if "WARN fs.FileSystem" not in line:
                    print line
            print ""

            if hc.get_version().startswith("Hadoop 2."):
                (stdout, stderr) = hc.execute("fs -du -s /", verbose=False)
                for line in stdout.splitlines():
                    if "WARN" not in line:
                        pos = line.rfind("\t")
                        size = int(line[:pos])
            else:
                (stdout, stderr) = hc.execute("fs -dus /", verbose=False)
                pos = stdout.rfind("\t")
                size = int(stdout[pos + 1:])

            human_readable_size = ""
            if 1024 < size < 1048576:
                human_readable_size = " (%.1f KB)" % (float(size)/1024)
            elif 1048576 < size < 1073741824:
                human_readable_size = " (%.1f MB)" % (float(size)/(1048576))
            elif size > 1073741824:
                human_readable_size = " (%.1f GB)" % (float(size)/(1073741824))

            print "Total Size = " + str(size) + human_readable_size + "\n"

        elif args.state == "dfs":
            (stdout, stderr) = hc.execute("dfsadmin -report", verbose=False)
            print ""
            for line in stdout.splitlines():
                if "WARN fs.FileSystem" not in line:
                    print line
            print ""

        elif args.state == "dfsblocks":
            (stdout, stderr) = hc.execute("fsck -blocks", verbose=False)
            print ""
            print stdout
            print ""

        elif args.state == "mrjobs":
            (stdout, stderr) = hc.execute("job -list all", verbose=False)
            print ""
            print stdout
            print ""

    serialize_cluster(ClusterType.HADOOP, hc_id, hc)