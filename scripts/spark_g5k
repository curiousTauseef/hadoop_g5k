#!/usr/bin/env python

import os
import sys

from argparse import ArgumentParser, RawTextHelpFormatter
from execo import Host

from execo.log import style
from execo_engine import logger

from hadoop_g5k.ecosystem.spark import SparkCluster, STANDALONE_MODE, YARN_MODE, \
    ScalaSparkJob, PythonSparkJob
from hadoop_g5k.util import generate_hosts, generate_new_id, ClusterType, \
    get_default_id, deserialize_cluster, remove_cluster, serialize_cluster, \
    cluster_exists, link_to_hadoop_cluster


if __name__ == "__main__":

    prog = "spark_g5k"
    description = "This tool helps you to manage a Spark cluster in Grid5000."
    parser = ArgumentParser(prog=prog,
                            description=description,
                            formatter_class=RawTextHelpFormatter,
                            add_help=False)

    actions = parser.add_argument_group(style.host("General options"),
                                        "Options to be used generally "
                                        "with Spark actions.")

    actions.add_argument("-h", "--help",
                         action="help",
                         help="Show this help message and exit")

    actions.add_argument("--id",
                         action="store",
                         nargs=1,
                         metavar="ID",
                         help="The identifier of the cluster. If not indicated"
                                ", last used cluster will be used (if any)")

    actions.add_argument("--node",
                         action="store",
                         nargs=1,
                         metavar="NODE",
                         help="Node where the action will be executed. Applies"
                                " only to --shell, --scala_job and --py_job")

    actions.add_argument("--libjars",
                         action="store",
                         nargs="+",
                         metavar="LIB_JARS",
                         help="A list of libraries to be used in job execution"
                                ". Applies only to --scala_job and --py_job")

    actions.add_argument("--main_class",
                         action="store",
                         nargs=1,
                         metavar="CLASS",
                         help="The class to be executed. Applies only to "
                              "--scala_job")

    verbose_group = actions.add_mutually_exclusive_group()

    verbose_group.add_argument("-v", "--verbose",
                               dest="verbose",
                               action="store_true",
                               help="Run in verbose mode")

    verbose_group.add_argument("-q", "--quiet",
                               dest="quiet",
                               action="store_true",
                               help="Run in quiet mode")

    object_group = parser.add_argument_group(style.host("Object management "
                                                        "options"),
                                             "Options to create and destroy "
                                             "Spark cluster objects")

    object_mutex_group = object_group.add_mutually_exclusive_group()

    object_mutex_group.add_argument("--create",
                                    metavar="MODE",
                                    nargs=1,
                                    action="store",
                                    help="Create the cluster object with the "
                                    "given mode")

    object_mutex_group.add_argument("--delete",
                                    dest="delete",
                                    action="store_true",
                                    help="Remove all files used by the cluster")

    object_group.add_argument("--properties",
                              dest="properties",
                              nargs=1,
                              action="store",
                              help="File containing the properties to be used "
                              "(INI file). Applies only to --create")

    object_group.add_argument("--nodes",
                              metavar="MACHINELIST",
                              dest="nodes",
                              nargs=1,
                              action="store",
                              help="Use the nodes in MACHINELIST file. Applies "
                                   "only to --create")

    object_group.add_argument("--hid",
                              metavar="ID",
                              dest="hid",
                              nargs=1,
                              action="store",
                              help="Link to the Hadoop cluster with the given "
                                   "identifier. Applies only to --create")

    object_group.add_argument("--bootstrap",
                              metavar="SPARK_TAR",
                              nargs=1,
                              action="store",
                              help="Install Spark in the cluster nodes taking"
                                   " into account the specified properties.\n"
                                   "SPARK_TAR defines the path of the .tgz file"
                                   " containing Spark binaries.")

    actions = parser.add_argument_group(style.host("Spark actions"),
                                        "Actions to execute in the Spark "
                                        "cluster. Several options can be "
                                        "indicated at the same time.\n"
                                        "The order of execution is fixed no "
                                        "matter the order used in the "
                                        "arguments: it follows the order\n"
                                        "of the options.")

    actions.add_argument("--initialize",
                         dest="initialize",
                         action="store_true",
                         help="Initialize cluster: Copy configuration")

    actions.add_argument("--start",
                         dest="start",
                         action="store_true",
                         help="Start the master and slaves")

    exec_group = actions.add_mutually_exclusive_group()

    exec_group.add_argument("--scala_job",
                            action="store",
                            nargs="+",
                            metavar=("LOCAL_JOB_PATH", "PARAM"),
                            help="Copy the Scala job to the cluster and execute"
                                 " it with the specified parameters")

    exec_group.add_argument("--py_job",
                            action="store",
                            nargs="+",
                            metavar=("LOCAL_JOB_PATH", "PARAM"),
                            help="Copy the Python job to the cluster and "
                                 "execute it with the specified parameters")

    exec_group.add_argument("--shell",
                            action="store",
                            nargs="?",
                            const="ipython",
                            metavar="LANGUAGE",
                            help="Start a shell in the given language "
                                 "(IPython if nothing specified)")

    actions.add_argument("--stop",
                         dest="stop",
                         action="store_true",
                         help="Stop the master and slaves")

    actions.add_argument("--clean",
                         dest="clean",
                         action="store_true",
                         help="Remove files created by Spark")

    args = parser.parse_args()

    # Get id
    if args.id:
        sc_id = int(args.id[0])
    else:
        if args.create:
            sc_id = generate_new_id(ClusterType.SPARK)
        else:
            sc_id = get_default_id(ClusterType.SPARK)
            if not sc_id:
                logger.error("There is no available cluster. You must create a"
                             " new one")
                sys.exit(os.EX_DATAERR)

    logger.info("Using id = " + str(sc_id) + " (SPARK)")

    verbose = True
    if args.quiet:
        verbose = False

    # Check node specification
    node_host = None
    if args.node:
        if not (args.shell or args.py_job or args.scala_job):
            logger.warn("--node only applies to shell, --scala_job or --py_job")
        else:
            node_host = Host(args.node[0])

    # Create or load object
    if args.create:

        if cluster_exists(ClusterType.SPARK, sc_id):
            logger.error("There is a Spark cluster with that id. You must "
                         "remove it before or chose another id")
            sys.exit(os.EX_DATAERR)

        if args.create[0].upper() == "STANDALONE":
            mode = STANDALONE_MODE
        elif args.create[0].upper() == "YARN":
            mode = YARN_MODE
        else:
            logger.error("Unrecognized mode. It should be either STANDALONE or"
                         "YARN.")
            sys.exit(os.EX_DATAERR)

        if not (args.nodes or args.hid):
            logger.error("Nodes in the cluster must be specified either "
                         "directly or indirectly through a Hadoop cluster.")
            sys.exit(os.EX_USAGE)

        hosts = generate_hosts(args.nodes[0]) if args.nodes else None

        if args.hid:
            hc_id = args.hid[0]

            # Deserialize HadoopCluster and link it
            hc = deserialize_cluster(ClusterType.HADOOP, hc_id)
            link_to_hadoop_cluster(ClusterType.SPARK, sc_id, hc_id)
        else:
            hc = None

        props = args.properties[0] if args.properties else None

        sc = SparkCluster(mode, config_file=props, hosts=hosts,
                          hadoop_cluster=hc)

    elif args.delete:

        # Clean
        sc = deserialize_cluster(ClusterType.SPARK, sc_id)
        if sc.initialized:
            logger.warn("The cluster needs to be cleaned before removed.")
            sc.clean()

        # Remove hc dump file
        logger.info("Removing hc dump file from cluster")
        remove_cluster(ClusterType.SPARK, sc_id)

        sys.exit(os.EX_OK)
    else:

        # Deserialize
        sc = deserialize_cluster(ClusterType.SPARK, sc_id)

    # Execute options
    if args.bootstrap:
        sc.bootstrap(args.bootstrap[0])

    if args.initialize:
        sc.initialize()

    if args.start:
        sc.start()

    if args.shell:
        node_host = args.node if args.node else None
        sc.start_shell(language=args.shell, node=node_host)
    elif args.scala_job or args.py_job:
        node_host = args.node if args.node else None
        libjars = args.libjars if args.libjars else None

        if args.scala_job:
            main_class = args.main_class[0] if args.main_class else None
            job = ScalaSparkJob(args.scala_job[0], args.scala_job[1:],
                                libjars, main_class)
        else:  # elif args.py_job:
            job = PythonSparkJob(args.py_job[0], args.py_job[1:], libjars)

        sc.execute_job(job, verbose=verbose, node=node_host)
        if job.success:
            print "Job finished successfully"
        else:
            print "Job finished with errors"

    if args.stop:
        sc.stop()

    if args.clean:
        sc.clean()

    serialize_cluster(ClusterType.SPARK, sc_id, sc)