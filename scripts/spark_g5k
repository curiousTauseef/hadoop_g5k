#!/usr/bin/env python

import os
import sys
import threading

from argparse import ArgumentParser, RawTextHelpFormatter
from pprint import pformat

from execo.action import Get, Put, TaktukRemote
from execo.host import Host
from execo.log import style
from execo.process import SshProcess
from execo_engine import logger
from execo_g5k import get_oar_job_nodes, get_oargrid_job_nodes

from hadoop_g5k.ecosystem.spark import SparkCluster, STANDALONE_MODE, YARN_MODE
from hadoop_g5k.util import generate_hosts, generate_new_id, ClusterType, \
    get_default_id, deserialize_cluster, remove_cluster, serialize_cluster, \
    cluster_exists, link_to_hadoop_cluster


if __name__ == "__main__":

    prog = "spark_g5k"
    description = "This tool helps you to manage a Spark cluster in Grid5000."
    parser = ArgumentParser(prog=prog,
                            description=description,
                            formatter_class=RawTextHelpFormatter,
                            add_help=False)

    actions = parser.add_argument_group(style.host("General options"),
                                        "Options to be used generally "
                                        "with Spark actions.")

    actions.add_argument("-h", "--help",
                         action="help",
                         help="Show this help message and exit")

    actions.add_argument("--id",
                         action="store",
                         nargs=1,
                         metavar="ID",
                         help="The identifier of the cluster. If not indicated"
                                ", last used cluster will be used (if any)")

    verbose_group = actions.add_mutually_exclusive_group()

    verbose_group.add_argument("-v", "--verbose",
                               dest="verbose",
                               action="store_true",
                               help="Run in verbose mode")

    verbose_group.add_argument("-q", "--quiet",
                               dest="quiet",
                               action="store_true",
                               help="Run in quiet mode")

    object_group = parser.add_argument_group(style.host("Object management "
                                                        "options"),
                                             "Options to create and destroy "
                                             "Spark cluster objects")

    object_mutex_group = object_group.add_mutually_exclusive_group()

    object_mutex_group.add_argument("--create",
                                    metavar="MODE",
                                    nargs=1,
                                    action="store",
                                    help="Create the cluster object with the "
                                    "given mode")

    object_mutex_group.add_argument("--delete",
                                    dest="delete",
                                    action="store_true",
                                    help="Remove all files used by the cluster")

    object_group.add_argument("--properties",
                              dest="properties",
                              nargs=1,
                              action="store",
                              help="File containing the properties to be used "
                              "(INI file). Applies only to --create")

    object_group.add_argument("--nodes",
                              metavar="MACHINELIST",
                              dest="nodes",
                              nargs=1,
                              action="store",
                              help="Use the nodes in MACHINELIST file. Applies "
                                   "only to --create")

    object_group.add_argument("--hid",
                              metavar="ID",
                              dest="hid",
                              nargs=1,
                              action="store",
                              help="Link to the Hadoop cluster with the given "
                                   "identifier. Applies only to --create")

    object_group.add_argument("--bootstrap",
                              metavar="SPARK_TAR",
                              nargs=1,
                              action="store",
                              help="Install Spark in the cluster nodes taking"
                                   " into account the specified properties.\n"
                                   "SPARK_TAR defines the path of the .tgz file"
                                   " containing Spark binaries.")

    actions = parser.add_argument_group(style.host("Spark actions"),
                                        "Actions to execute in the Spark "
                                        "cluster. Several options can be "
                                        "indicated at the same time.\n"
                                        "The order of execution is fixed no "
                                        "matter the order used in the "
                                        "arguments: it follows the order\n"
                                        "of the options.")

    actions.add_argument("--initialize",
                         dest="initialize",
                         action="store_true",
                         help="Initialize cluster: Copy configuration")

    actions.add_argument("--start",
                         dest="start",
                         action="store_true",
                         help="Start the master and slaves")

    actions.add_argument("--shell",
                         dest="shell",
                         action="store_true",
                         help="Start a shell in Python")

    actions.add_argument("--stop",
                         dest="stop",
                         action="store_true",
                         help="Stop the master and slaves")

    actions.add_argument("--clean",
                         dest="clean",
                         action="store_true",
                         help="Remove files created by Spark")

    args = parser.parse_args()

    # Get id
    if args.id:
        sc_id = int(args.id[0])
    else:
        if args.create:
            sc_id = generate_new_id(ClusterType.SPARK)
        else:
            sc_id = get_default_id(ClusterType.SPARK)
            if not sc_id:
                logger.error("There is no available cluster. You must create a"
                             " new one")
                sys.exit(3)

    logger.info("Using id = " + str(sc_id) + " (SPARK)")

    verbose = True
    if args.quiet:
        verbose = False

    # Create or load object
    if args.create:

        if cluster_exists(ClusterType.SPARK, sc_id):
            logger.error("There is a Spark cluster with that id. You must "
                         "remove it before or chose another id")
            sys.exit(1)

        if args.create[0].upper() == "STANDALONE":
            mode = STANDALONE_MODE
        elif args.create[0].upper() == "YARN":
            mode = YARN_MODE
        else:
            logger.error("Unrecognized mode. It should be either STANDALONE or"
                         "YARN.")

        if not (args.nodes or args.hid):
            logger.error("Nodes in the cluster must be specified either "
                         "directly or indirectly through a Hadoop cluster.")
            sys.exit(2)

        hosts = generate_hosts(args.nodes[0]) if args.nodes else None

        if args.hid:
            hc_id = args.hid[0]

            # Deserialize HadoopCluster and link it
            hc = deserialize_cluster(ClusterType.HADOOP, hc_id)
            link_to_hadoop_cluster(ClusterType.SPARK, sc_id, hc_id)
        else:
            hc = None

        props = args.properties[0] if args.properties else None

        sc = SparkCluster(mode, config_file=props, hosts=hosts,
                          hadoop_cluster=hc)

    elif args.delete:

        # Clean
        sc = deserialize_cluster(ClusterType.SPARK, sc_id)
        if sc.initialized:
            logger.warn("The cluster needs to be cleaned before removed.")
            sc.clean()

        # Remove hc dump file
        logger.info("Removing hc dump file from cluster")
        remove_cluster(ClusterType.SPARK, sc_id, sc)

        sys.exit(0)
    else:

        # Deserialize
        sc = deserialize_cluster(ClusterType.SPARK, sc_id)

    # Execute options
    if args.bootstrap:
        sc.bootstrap(args.bootstrap[0])

    if args.initialize:
        sc.initialize()

    if args.start:
        sc.start()

    if args.shell:
        sc.start_shell()

    if args.stop:
        sc.stop()

    if args.clean:
        sc.clean()

    serialize_cluster(ClusterType.SPARK, sc_id, sc)